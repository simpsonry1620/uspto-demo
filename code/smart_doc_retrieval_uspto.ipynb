{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6bd4f0-3d46-4da1-a101-22dfd61a9fe1",
   "metadata": {},
   "source": [
    "# Smart Document Retrieval\n",
    "<p>The primary focus of the notebook is to illustrate the process of using a transformer model to embed text data into a numerical representation that can be used to calculated a similarity score as compared to a query string embedding.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c2c8fc-dc8a-45a2-85bc-86dcf7ca8c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332f8c1d-78af-4d6d-b55a-1d731149288f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Import SentenceTransformer and util from the HuggingFace sentence_transformer library which has\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# been pre-installed in this environment.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Import pickle. pickle is used to store the embedding\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "## Imports and dependencies\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "# Importing the needed libraries & Modules\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Import SentenceTransformer and util from the HuggingFace sentence_transformer library which has\n",
    "# been pre-installed in this environment.\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Import pickle. pickle is used to store the embedding\n",
    "import pickle\n",
    "\n",
    "# Import Path. Used to manage file system\n",
    "from pathlib import Path\n",
    "\n",
    "# Import smart_search_models. This module was created for this example to simplify the management of the \n",
    "# various models that can be used for the embedding process.\n",
    "import smart_search\n",
    "\n",
    "def get_txt_files_in_folder(folder_path):\n",
    "    txt_files = glob.glob(folder_path + '/*.txt')\n",
    "    return txt_files\n",
    "\n",
    "def read_file_lines(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            lines = [line.strip() for line in lines]\n",
    "            return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filename}' not found.\")\n",
    "        return []  \n",
    "\n",
    "def extract_info_from_txt(txt_path):\n",
    "    with open(txt_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    doc_number = re.search(r'Document Number: (\\d+)', content).group(1)\n",
    "    date = re.search(r'Date: (\\d+)', content).group(1)\n",
    "    title = re.search(r'Title: (.+)', content).group(1)\n",
    "    abstract = re.search(r'Abstract: (.+)', content).group(1)\n",
    "\n",
    "    return {\n",
    "        'Document Number': doc_number,\n",
    "        'Date': date,\n",
    "        'Title': title,\n",
    "        'Abstract': abstract\n",
    "    }\n",
    "\n",
    "def create_dataframe_from_txt_files(file_paths):\n",
    "    data = [extract_info_from_txt(file_path) for file_path in file_paths]\n",
    "    df = pd.DataFrame(data)\n",
    "    return df    \n",
    "\n",
    "# Set some notebook variables\n",
    "DATASET_NAME = \"uspto\"\n",
    "EMBEDDING_FOLDER = \"embeddings/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1556ad-8ef2-4c04-8d22-229c396760d4",
   "metadata": {},
   "source": [
    "# Source Text Storage\n",
    "<p>The example dataset used in this notebook has been stored in a collection of abstracts extracted from the patent-grant-full-text-dataxml dataset. The abstracts were stored as plain text files, then imported into a dataframe and stored in a parquet file.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8e6913-d9eb-4701-baf0-6ddd69ddd088",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 534169 files.\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe of abstracts\n",
    "folder_path = \"./output/abstracts/\"\n",
    "abs_files = get_txt_files_in_folder(folder_path)\n",
    "\n",
    "print(f\"There are {len(abs_files)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58dd284-a6b1-4df7-86b1-f5334672ae21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11276134</td>\n",
       "      <td>20220315</td>\n",
       "      <td>Reconfigurable image processing hardware pipeline</td>\n",
       "      <td>A reconfigurable image processing pipeline in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11324814</td>\n",
       "      <td>20220510</td>\n",
       "      <td>Live attenuated oral vaccine against ETEC and ...</td>\n",
       "      <td>Disclosed is the attenuated Salmonella typhi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11508069</td>\n",
       "      <td>20221122</td>\n",
       "      <td>Method for processing event data flow and comp...</td>\n",
       "      <td>The present disclosure provides a method for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11304408</td>\n",
       "      <td>20220419</td>\n",
       "      <td>Leash attachment</td>\n",
       "      <td>A leash attachment and method for using the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11383015</td>\n",
       "      <td>20220712</td>\n",
       "      <td>System and method for plasma purification prio...</td>\n",
       "      <td>A method of collecting mononuclear cells incl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document Number      Date  \\\n",
       "0        11276134  20220315   \n",
       "1        11324814  20220510   \n",
       "2        11508069  20221122   \n",
       "3        11304408  20220419   \n",
       "4        11383015  20220712   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Reconfigurable image processing hardware pipeline   \n",
       "1  Live attenuated oral vaccine against ETEC and ...   \n",
       "2  Method for processing event data flow and comp...   \n",
       "3                                   Leash attachment   \n",
       "4  System and method for plasma purification prio...   \n",
       "\n",
       "                                            Abstract  \n",
       "0   A reconfigurable image processing pipeline in...  \n",
       "1   Disclosed is the attenuated Salmonella typhi ...  \n",
       "2   The present disclosure provides a method for ...  \n",
       "3   A leash attachment and method for using the l...  \n",
       "4   A method of collecting mononuclear cells incl...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the DataFrame\n",
    "abstract_dataframe_path = 'uspto_abstracts.parquet'\n",
    "abstract_file = Path(abstract_dataframe_path)\n",
    "\n",
    "if abstract_file.is_file():\n",
    "    df = pd.read_parquet(abstract_dataframe_path)\n",
    "else:    \n",
    "    print('Did not find abstract file.')\n",
    "    df = create_dataframe_from_txt_files(abs_files)\n",
    "    df.to_parquet('uspto_abstracts.parquet')\n",
    "    df.head()\n",
    "    \n",
    "df.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244c2664-0415-4735-ac91-556473c989e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Number: 11276134\n",
      "Date: 20220315\n",
      "Title: Reconfigurable image processing hardware pipeline\n",
      "Abstract:  A reconfigurable image processing pipeline includes an image signal processor (ISP), a control processor, and a local memory. ISP processes raw pixel data for a frame based on an image processing parameter and provides lines of processed pixel data to control processor via a first interface. For each region of interest (ROI) in the frame, ISP generates auto-exposure and auto-white balance (2A) statistics based on the lines for the ROI and writes them to the local memory via a second interface. Control processor reads 2A statistics from the local memory, determines the image processing parameter based on them, and provides the image processing parameter to ISP. ISP also generates an integer N bin histogram for control processor, which sums a portion of the N total bins and compares the summed bin count to a lighting transition threshold. The image processing parameter is further based on the comparison.\n"
     ]
    }
   ],
   "source": [
    "abs_txt = read_file_lines(abs_files[0])    \n",
    "# Show example document\n",
    "\n",
    "for line in abs_txt:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d522c0a4-64de-45a1-a2d4-61ad96835051",
   "metadata": {},
   "source": [
    "## Source Text Embedding\n",
    "<p>Historical methods for search involved simple <a href='https://en.wikipedia.org/wiki/Lexicography'>lexicographical</a> similarity pattern matching such as <a href='https://en.wikipedia.org/wiki/Regular_expression'>regex</a>. Although methods such as lexical search can be useful for some use cases, they have several disadvantages such as needing to specific the precise terms to search for. To improve search results it can be advantageous to search based on <a href='https://en.wikipedia.org/wiki/Semantic_similarity#:~:text=Semantic%20similarity%20is%20a%20metric,as%20opposed%20to%20lexicographical%20similarity.'>sematic similarity</a> using concepts rather than word for comparison.</p>\n",
    "\n",
    "<p>To be able to search by concept we must be able to represent our data in the form of concepts. This is where <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> come in. <a href='https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)'>Transformers</a> are a form of Machine Learning that can be applied to Natural Language Processing (NLP), the models have been trained on extremely large datasets such as Wikipedia to develop the ability to represent input text as a highly dimensional numerical representation, this process is called <a href='https://vaclavkosar.com/ml/transformer-embeddings-and-tokenization'>embedding</a>. If this sounds complicated, don't worry the hard parts are all abstracted away for us, we just need to use the sentence transformer library. Although there are benefits of understanding how the models work, sometimes it can be just as valuable to show how easy they are to use and how impressive the results can be using off-the-shelf models. If greater accuracy is needed you can always <a href='https://www.sbert.net/docs/training/overview.html'>train transformers</a> on your own datasets to improve their capabilities.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e3398-8aa2-49c4-b71f-f4da717f990d",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "<p> There are a large number of models to choose from on <a href='https://huggingface.co/'>HuggingFace</a> even for just the task of <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a>(>800 as of 11/2022). We have included a python module to help simplify organization and selection of a smaller subset of models to experiment with (~100). Using <a href='https://huggingface.co/'>HuggingFace</a> simplifies the process of downloading and running the various models, it is not the only way to consume Transformers but it was chosen as it is one of the easiest ways to get started.</p>\n",
    "    \n",
    "There are several areas to consider when selecting a model for a given task\n",
    "<li><b>Model Size</b> - Large models need more VRAM and can take longer to run but may be more 'accurate'</li>\n",
    "<li><b>Model Architecture</b> - Some models might be designed for specific use cases or finetuned for a given problem. If your use case is similar, you might have high performance out of the box.</li>\n",
    "<li><b>Task</b> - Different models have been trained for different tasks. Some examples of various tasks include; Semantic Similarity, Semantic Search, Questioning and Answering, and Document Summarization. \n",
    "    \n",
    "<p>As stated above, the models have been trained to solve a specific workflow. In our case we are trying to identify Semantically Similar documents to our query string. Within the Semantic Similarity group there are subgroups of tasks. These tasks include identifying semantically similar sentences where we try to evaluate two or more sentences and score their similarity. When the elements being evaluated are of similar length (sentence to sentence, paragraph to parapraph) the process is called <b>symmetric semantic search</b>. If you are evaluating a short query phrase or word to sentance, paragraphs, or even documents it is refered to as <b>asymmetric semantic search</b> and models have been specially trained for each type.</p>\n",
    "    \n",
    "<li><a href='https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models'>Symmetric Semantic Search Pretrained Models</a></li>\n",
    "<li><a href='https://www.sbert.net/docs/pretrained-models/msmarco-v3.html'>Asymmetric Semantic Search</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6d366e-220d-4917-9395-a40fdc9dba92",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the Model\n",
    "<p>Loading the model is a simple as passing the model's name as an input argument to create a model object. If the model isn't available locally it will be downloaded automatically. One of the hardest parts of working with HuggingFace is keeping track of all the models available. You can view all the models available for <a href='https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads'>Sentence Similarity</a> and copy the name into the code or to simplify things we have created a very basic python module <a href='smart_search.py'>smart_search.py</a> to hold model names.</p>\n",
    "\n",
    "<details>\n",
    "  <summary>SentenceTransformer Parameters</summary>\n",
    "<li><b>model_name_or_path</b> – If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from Huggingface models repository with that name.</li>\n",
    "<li><b>modules</b> – This parameter can be used to create custom SentenceTransformer models from scratch.</li>\n",
    "<li><b>device</b> – Device (like ‘cuda’ / ‘cpu’) that should be used for computation. If None, checks if a GPU can be used.</li>\n",
    "<li><b>cache_folder</b> – Path to store models</li>\n",
    "<li><b>use_auth_token</b> – HuggingFace authentication token to download private models.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d3181b-83c9-4868-80f9-e88194092c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: 'msmarco-distilbert-base-v4'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9e55f6ed4c40b08729d0c71f141faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2aac748a474ce994c75e89fe6d4162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c227ce9c4e44cab84b81235778b1836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e2625bd5224599b5309049c95f294e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb781dfb689b4e00acea4bdba946d5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8afc6c8272489a9f6ebdfe1cc1ede9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f84ff933204450b602e1d3d440d9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661b9e91390f43cdb572ed545cc1d97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bf350a52c14f58824e122d1c3a4770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd8c4fb554041edbc0b26a632a6879a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cfc286d256412faea601d60f2dac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select and load model.\n",
    "# Note: If a given model hasn't been used since the container has been loaded it will be downloaded automatically.\n",
    "\n",
    "# The sentence_models list is a large list of models. They have not been grouped by task beyond sentence similarity \n",
    "#model_name = smart_search.sentence_models[3]\n",
    "#model_name = smart_search_models.default_model\n",
    "\n",
    "# asymmetric_cosine_similarity_models are special purpose models for Asymmetric Semantic Similarity through cosine similarity calculations\n",
    "model_name = smart_search.asymmetric_cosine_similarity_models[0]\n",
    "\n",
    "# symmetric_models are special purpose models for Symmetric Sematic Similarity\n",
    "#model_name = smart_search.symmetric_models[3]\n",
    "\n",
    "print(\"Loading model: '{}'\".format(model_name))\n",
    "#model = SentenceTransformer(model_name,cache_folder='./models/', device='cpu')\n",
    "model = SentenceTransformer(model_name,cache_folder='./models/', device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c4a8b-190e-46c2-9af3-d1b37a0f6aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Source Text Embedding\n",
    "<p>To embed the source text, we can pass the entire column of our dataset into the model object in a single line of code as shown in the cell blocks below.</p>\n",
    "\n",
    "<p>A couple important items to note here:\n",
    "    <li>You only need to embed the source text once for a given model. Depending on your use case you may wish to database the embeddings for later use, just remember to keep track of the model used for embedding and the source document.</li>\n",
    "    <li>As each model will embed the input text differently you need to ensure the source text and query text were embedded using the same model. If you choose to database or store your embedding for later just be sure to track which models were used for the embedding as you will likely get unexpected results if comparing embedding from different models.</li>\n",
    "    </p>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>encode Parameters</summary>\n",
    "    <li><b>sentences</b> – the sentences to embed</li>\n",
    "    <li><b>batch_size</b> – the batch size used for the computation</li>\n",
    "    <li><b>show_progress_bar</b> – Output a progress bar when encode sentences</li>\n",
    "    <li><b>output_value</b> – Default sentence_embedding, to get sentence embeddings. Can be set to token_embeddings to get wordpiece token embeddings. Set to None, to get all output values</li>\n",
    "    <li><b>convert_to_numpy</b> – If true, the output is a list of numpy vectors. Else, it is a list of pytorch tensors.</li>\n",
    "    <li><b>convert_to_tensor</b> – If true, you get one large tensor as return. Overwrites any setting from convert_to_numpy</li>\n",
    "    <li><b>device</b> – Which torch.device to use for the computation</li>\n",
    "    <li><b>normalize_embeddings</b> – If set to true, returned vectors will have length 1. In that case, the faster dot-product (util.dot_score) instead of cosine similarity can be used.</li>\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e3b37-71b8-42a0-b398-ee75d7f3471f",
   "metadata": {},
   "source": [
    "<p>In the cell below we call the encoder for every message. This does not take advantage parrallel processing and we can see the processing time difference. The timings are a result of test runs using an NVIDIA RTX A6000.</p>\n",
    "<li>Example Size: 534,169 Patents</li>\n",
    "\n",
    "<b>asymmetric_cosine_similarity_models</b>\n",
    "<li>Model: msmarco-distilbert-base-v4</li>\n",
    "<li>Wall Time: 12min 45s</li>\n",
    "</br>\n",
    "<li>Model: msmarco-roberta-base-v3</li>\n",
    "<li>Example Size: 534,169 Patents</li>\n",
    "<li>Wall Time: 23min 4s</li>\n",
    "</br>\n",
    "<li>Model: msmarco-distilbert-base-v3</li>\n",
    "\n",
    "<li>Wall Time: 12min 16s</li>\n",
    "</br>\n",
    "<b>symmetric_cosine_similarity_models</b>\n",
    "<li>Model: all-mpnet-base-v2</li>\n",
    "<li>Wall Time: 24min 47s</li>\n",
    "</br>\n",
    "<li>Model: multi-qa-mpnet-base-dot-v1</li>\n",
    "<li>Wall Time: </li>\n",
    "</br>\n",
    "<li>Model: all-distilroberta-v1</li>\n",
    "<li>Wall Time: </li>\n",
    "</br>\n",
    "<li>Model: all-MiniLM-L12-v2</li>\n",
    "<li>Wall Time: </li>\n",
    "</br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f289d0e-c1d9-47f5-8b0c-15b8a2314eae",
   "metadata": {},
   "source": [
    "### Embedding the entire dataset\n",
    "We only need to embed the entire dataset once. We can check if the model / dataset embeddings already exist. If so just load them from disk. If not, process them. This can take as long as 30 minutes to embed the ~500,000 patents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5c8345-4afa-4716-95fb-cefe3a2ae06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create helper functions to read and write embedding to files.\n",
    "def load_embeddings(embedding_file_path):\n",
    "        \n",
    "    #Load sentences & embeddings from disc\n",
    "    with open(embedding_file_path, \"rb\") as fIn:\n",
    "        stored_data = pickle.load(fIn)\n",
    "        stored_message_id = stored_data['document']\n",
    "        stored_embeddings = stored_data['embeddings']\n",
    "\n",
    "    # As of now we only need the stored embeddings\n",
    "    return stored_embeddings\n",
    "\n",
    "def write_embeddings(embedding_folder, embedding_file_name,message_ids,source_embeddings):\n",
    "   \n",
    "    # Check if directory exits\n",
    "    dir_path = Path(embedding_folder)\n",
    "    \n",
    "    if not dir_path.is_dir():\n",
    "        print(\"Directory does not exist. Creating it now.\")\n",
    "        # If the directory doesn't exist create it.\n",
    "        dir_path.mkdir()\n",
    "        \n",
    "    # Create the file path\n",
    "    file_path = embedding_folder + embedding_file_name\n",
    "    \n",
    "    # Write out the embedding and message_id to disk\n",
    "    with open(file_path, \"wb\") as fOut:\n",
    "        pickle.dump({'document': message_ids, 'embeddings': source_embeddings}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1b71a7-1f7a-4666-b7cb-fd78f261eae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file exists. Loading it now.\n",
      "embeddings/embeddings_uspto_msmarco-distilbert-base-v4.pkl\n",
      "CPU times: user 407 ms, sys: 2.38 s, total: 2.79 s\n",
      "Wall time: 2.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the file name that would be used to store the embeddings.\n",
    "embedding_file_name = \"embeddings_{}_{}.pkl\".format(DATASET_NAME,model_name)\n",
    "\n",
    "# Create embedding Path object\n",
    "embedding_file = Path(EMBEDDING_FOLDER + embedding_file_name)\n",
    "\n",
    "# Check if the file \n",
    "if embedding_file.is_file():\n",
    "    # If a file exists with the embedding file for this dataset / model combination exists load it.\n",
    "    print(\"Embedding file exists. Loading it now.\")\n",
    "    source_embeddings = load_embeddings(embedding_file)\n",
    "else:\n",
    "    # If an embedding file does not exist. Embed the dataset and cache the data.\n",
    "    print(\"Embedding file does not exist. Creating now.\")\n",
    "    \n",
    "    source_embeddings = model.encode(df.Abstract,convert_to_tensor=True,show_progress_bar=True)\n",
    "    \n",
    "    # Write out the generated embeddings\n",
    "    write_embeddings(EMBEDDING_FOLDER,embedding_file_name,df.Abstract,source_embeddings)\n",
    "    \n",
    "print(embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b6bc1-f0a3-4c1f-946f-529bba635dcd",
   "metadata": {},
   "source": [
    "## 6) Query String Embedding\n",
    "<p>Using the same model, we then embed our query string to be used for comparison.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c44b46-0319-4f84-8f58-d417b5684428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An anomaly detector includes a writing unit that writes anomaly detection data readable by an external diagnostic device to an external memory when an anomaly is detected in an on-board device. Further, the anomaly detector includes a determination unit that determines whether a failure is occurring in a memory, which is used when a processor is operated during the writing unit performs the writing. Also, the anomaly detector includes a resetting unit that resets the memory by activating a specified one of reset functions of the processor when the determination unit determines that a failure is occurring in the memory. When the determination unit determines that a failure is occurring in the memory, the writing unit writes the anomaly detection data after the memory is reset by the specified one of the reset functions. \n",
      "CPU times: user 295 ms, sys: 125 ms, total: 420 ms\n",
      "Wall time: 414 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Embed the query string\n",
    "#query_string = 'Artificial intelligence (AI) anomaly monitoring in a storage system. The AI anomaly monitoring may include writing commands into a log jointly with the execution of the commands on storage media of a drive. The log includes information regarding the operation of the drive including, at least, the commands. In turn, each drive in the storage system may include an AI processor core that may access the log and apply an AI analysis to the log to monitor for an anomaly regarding the operation of the drive. As each drive in the storage system may use the AI process core to detect anomalies locally to the drive, the computational and network resources needed to employ the AI monitoring may be reduced.'\n",
    "query_string = \"An anomaly detector includes a writing unit that writes anomaly detection data readable by an external diagnostic device to an external memory when an anomaly is detected in an on-board device. Further, the anomaly detector includes a determination unit that determines whether a failure is occurring in a memory, which is used when a processor is operated during the writing unit performs the writing. Also, the anomaly detector includes a resetting unit that resets the memory by activating a specified one of reset functions of the processor when the determination unit determines that a failure is occurring in the memory. When the determination unit determines that a failure is occurring in the memory, the writing unit writes the anomaly detection data after the memory is reset by the specified one of the reset functions. \"\n",
    "query_embedding = model.encode(query_string,convert_to_tensor=True)\n",
    "\n",
    "print(query_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722045ab-8aad-4303-88ae-4663229769c5",
   "metadata": {},
   "source": [
    "## 7) Similarity Scoring and Ranking\n",
    "<p>Next, we need to calculate the similarity between the query embedding and all the source text embeddings. One of the most common approaches is to calculate the cosine similarity. Again, the complexities and math have been abstracted here with the <a href='https://www.sbert.net/docs/package_reference/util.html'>util.cos_sim</a> and sematic_search functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d1fbabb-c332-44e6-a660-895343288bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 ms, sys: 2.9 ms, total: 20.9 ms\n",
      "Wall time: 19.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set k as the number of top results\n",
    "k = 50\n",
    "\n",
    "# Using the util function to run semantic search, default to cosine\n",
    "topk_results = util.semantic_search(query_embedding, source_embeddings, top_k=k)[0]\n",
    "\n",
    "# Extract the result ids\n",
    "topk_results_ids = [result['corpus_id'] for result in topk_results]\n",
    "\n",
    "# Get a dataframe of the top k results\n",
    "topk_df = df.iloc[topk_results_ids].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e980eec-aff4-4d81-8d97-2be655c4c1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Document Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>371003</td>\n",
       "      <td>11379310</td>\n",
       "      <td>20220705</td>\n",
       "      <td>Anomaly detector</td>\n",
       "      <td>An anomaly detector includes a writing unit t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>370470</td>\n",
       "      <td>11283705</td>\n",
       "      <td>20220322</td>\n",
       "      <td>Anomaly detector, anomaly detection network, m...</td>\n",
       "      <td>An anomaly detector (100) for detecting an ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>405805</td>\n",
       "      <td>11423698</td>\n",
       "      <td>20220823</td>\n",
       "      <td>Anomaly detector for detecting anomaly using c...</td>\n",
       "      <td>Embodiments of the present disclosure disclos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>448866</td>\n",
       "      <td>11333580</td>\n",
       "      <td>20220517</td>\n",
       "      <td>Anomaly detecting device, anomaly detection me...</td>\n",
       "      <td>An anomaly detecting device includes: a singu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>503675</td>\n",
       "      <td>11520672</td>\n",
       "      <td>20221206</td>\n",
       "      <td>Anomaly detection device, anomaly detection me...</td>\n",
       "      <td>An anomaly detection device according to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index Document Number      Date  \\\n",
       "0  371003        11379310  20220705   \n",
       "1  370470        11283705  20220322   \n",
       "2  405805        11423698  20220823   \n",
       "3  448866        11333580  20220517   \n",
       "4  503675        11520672  20221206   \n",
       "\n",
       "                                               Title  \\\n",
       "0                                   Anomaly detector   \n",
       "1  Anomaly detector, anomaly detection network, m...   \n",
       "2  Anomaly detector for detecting anomaly using c...   \n",
       "3  Anomaly detecting device, anomaly detection me...   \n",
       "4  Anomaly detection device, anomaly detection me...   \n",
       "\n",
       "                                            Abstract  \n",
       "0   An anomaly detector includes a writing unit t...  \n",
       "1   An anomaly detector (100) for detecting an ab...  \n",
       "2   Embodiments of the present disclosure disclos...  \n",
       "3   An anomaly detecting device includes: a singu...  \n",
       "4   An anomaly detection device according to the ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89de9eb7-a36d-41b2-81d4-fc1cfb391d71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 11379310\n",
      "Title: Anomaly detector\n",
      "Abstract:  An anomaly detector includes a writing unit that writes anomaly detection data readable by an external diagnostic device to an external memory when an anomaly is detected in an on-board device. Further, the anomaly detector includes a determination unit that determines whether a failure is occurring in a memory, which is used when a processor is operated during the writing unit performs the writing. Also, the anomaly detector includes a resetting unit that resets the memory by activating a specified one of reset functions of the processor when the determination unit determines that a failure is occurring in the memory. When the determination unit determines that a failure is occurring in the memory, the writing unit writes the anomaly detection data after the memory is reset by the specified one of the reset functions. \n",
      "\n",
      "Document: 11283705\n",
      "Title: Anomaly detector, anomaly detection network, method for detecting an abnormal activity, model determination unit, system, and method for determining an anomaly detection model\n",
      "Abstract:  An anomaly detector (100) for detecting an abnormal activity in a telecommunications network. The anomaly detector (100) includes a receiver unit (102) configured to receive a plurality of network measurements (nm) in relation to the telecommunications network; a model-based mapping unit (104) configured to map the received plurality of network measurements (nm) via a machine-trained anomaly detection model (106) to an anomaly indicator (ai); and an output unit (108) configured to provide the anomaly indicator (ai), which indicates the abnormal activity in the telecommunications network. \n",
      "\n",
      "Document: 11423698\n",
      "Title: Anomaly detector for detecting anomaly using complementary classifiers\n",
      "Abstract:  Embodiments of the present disclosure disclose an anomaly detector for detecting an anomaly in a sequence of poses of a human performing an activity. The anomaly detector includes an input interface configured to accept input data indicative of a distribution of the sequence of poses, a memory configured to store a discriminative one-class classifier having a pair of complementary classifiers bounding normal distribution of pose sequences in a reproducing kernel Hilbert space (RKHS), a processor configured to embed the input data into an element of the RKHS and classify the embedded data using the discriminative one-class classifier, and an output interface configured to render a classification result. \n",
      "\n",
      "Document: 11333580\n",
      "Title: Anomaly detecting device, anomaly detection method and program\n",
      "Abstract:  An anomaly detecting device includes: a singular value decomposition unit configured to perform singular value decomposition of a variance-covariance matrix of a measured value matrix y0 composed of measured values acquired by a plurality of sensors in a time period considered to be normal, to thereby calculate a singular vector U and a singular value matrix S; an anomaly determination unit configured to apply the singular vector U and the singular value matrix S to a measured value matrix yt to be evaluated and which is acquired in an arbitrary time period to determine whether an anomaly is present from a result of application; and an anomalous part identification unit configured to, when the measured value matrix yt is determined to be anomalous, identify an anomalous part based on a diagonal element of a matrix obtained in association with the measured value matrix yt. \n",
      "\n",
      "Document: 11520672\n",
      "Title: Anomaly detection device, anomaly detection method and storage medium\n",
      "Abstract:  An anomaly detection device according to the embodiment includes a prediction unit and an anomaly score calculation unit. The prediction unit performs a process to obtain, at each time step of the time series data of m dimensions, distribution parameters required to express a continuous probability distribution representing a distribution state of predicted values that can be obtained at a time step t of the time series data of m dimensions. The anomaly score calculation unit performs a process to calculate, using distribution parameters obtained by the prediction unit, an anomaly score corresponding to an evaluation value representing evaluation of a magnitude of anomaly in an actual measurement value at the time step t of time series data of m dimensions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Document: {topk_df['Document Number'][i]}\")\n",
    "    print(f\"Title: {topk_df['Title'][i]}\")\n",
    "    print(f\"Abstract: {topk_df['Abstract'][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a7017-3256-4da1-b08a-c011fffebda1",
   "metadata": {},
   "source": [
    "## Advanced Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb860166-738c-455e-bb44-5f029910bdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55162e46b95b4c68bc3207772177d79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce09e5077e1f42f0a97efae2142ade09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6543b1f09c4efc94fe5df971de232c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2963ee29e1264cb989f00f5e8f87ca3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5cb67b7db04330872d162c32b8bf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the cross encoder library\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# Load the cross encoder model\n",
    "cross_model = CrossEncoder(smart_search.cross_encoder_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5b121a-a62b-4b87-9aae-8099ebda388b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 355 ms, sys: 21.1 ms, total: 376 ms\n",
      "Wall time: 374 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate the cross-encoder scores and assign scores to dataframe column\n",
    "topk_df['score'] = [cross_model.predict([query_string,msg]) for msg in topk_df.Abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5275255a-8329-4130-9c82-98a1f7023b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort the dataframe based on descending score\n",
    "topk_df = topk_df.sort_values('score',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e4268fb-4ce2-4e4a-af20-c570f5155d8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 11379310\n",
      "Title: Anomaly detector\n",
      "Abstract:  An anomaly detector includes a writing unit that writes anomaly detection data readable by an external diagnostic device to an external memory when an anomaly is detected in an on-board device. Further, the anomaly detector includes a determination unit that determines whether a failure is occurring in a memory, which is used when a processor is operated during the writing unit performs the writing. Also, the anomaly detector includes a resetting unit that resets the memory by activating a specified one of reset functions of the processor when the determination unit determines that a failure is occurring in the memory. When the determination unit determines that a failure is occurring in the memory, the writing unit writes the anomaly detection data after the memory is reset by the specified one of the reset functions. \n",
      "\n",
      "Document: 11580005\n",
      "Title: Anomaly pattern detection system and method\n",
      "Abstract:  Provided is an anomaly pattern detection system including an anomaly detection device connected to one or more servers. The anomaly detection device may include an anomaly detector configured to model input data by considering all of the input data as normal patterns, and detect an anomaly pattern from the input data based on the modeling result. \n",
      "\n",
      "Document: 11283705\n",
      "Title: Anomaly detector, anomaly detection network, method for detecting an abnormal activity, model determination unit, system, and method for determining an anomaly detection model\n",
      "Abstract:  An anomaly detector (100) for detecting an abnormal activity in a telecommunications network. The anomaly detector (100) includes a receiver unit (102) configured to receive a plurality of network measurements (nm) in relation to the telecommunications network; a model-based mapping unit (104) configured to map the received plurality of network measurements (nm) via a machine-trained anomaly detection model (106) to an anomaly indicator (ai); and an output unit (108) configured to provide the anomaly indicator (ai), which indicates the abnormal activity in the telecommunications network. \n",
      "\n",
      "Document: 11425005\n",
      "Title: Controller area network anomaly detection\n",
      "Abstract:  An anomaly detector of a Controller Area Network (CAN) bus performs analysis on messages received from the CAN bus to determine if the messages are anomalous. The anomaly detector may be implemented on a vehicle by an Electronic Control Unit (ECU). The anomaly detector may extract a batch of feature vectors for binary messages received from the CAN bus. The anomaly detector then performs a model adaption to adapt a previous probability model with the batch of feature vectors. The adapted probability model is then compared with a universal background model to determine a network anomaly level. \n",
      "\n",
      "Document: 11689434\n",
      "Title: Network anomaly detection\n",
      "Abstract:  An anomaly detector of a Controller Area Network (CAN) bus performs analysis on messages received from the CAN bus to determine if the messages are anomalous. The anomaly detector may be implemented on a vehicle by an Electronic Control Unit (ECU). The anomaly detector may extract a batch of feature vectors for binary messages received from the CAN bus. The anomaly detector then performs a model adaption to adapt a previous probability model with the batch of feature vectors. The adapted probability model is then compared with a universal background model to determine a network anomaly level. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Document: {topk_df['Document Number'][i]}\")\n",
    "    print(f\"Title: {topk_df['Title'][i]}\")\n",
    "    print(f\"Abstract: {topk_df['Abstract'][i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82095e-fb90-4b61-89bf-35ed15940155",
   "metadata": {},
   "source": [
    "# Identification of most similar sentence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10d26f9d-2cea-43bd-9daf-ece6281cb482",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29033e94-3b05-438a-9bb8-8c9ec414307b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/rapids/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/rapids/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/rapids/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cb5d79b-94f6-4759-b957-508a413462fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    filtered_tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def compute_tfidf(sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    return vectorizer.fit_transform(sentences)\n",
    "\n",
    "def compute_similarity(vector1, vector2):\n",
    "    return cosine_similarity(vector1, vector2)[0][0]\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    preprocessed_sentences = [preprocess(sentence) for sentence in [sentence1, sentence2]]\n",
    "    tfidf_vectors = compute_tfidf(preprocessed_sentences)\n",
    "    similarity = compute_similarity(tfidf_vectors[0], tfidf_vectors[1])\n",
    "    return similarity\n",
    "\n",
    "def split_paragraph_into_sentences(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bd0b08d-7958-4b7c-a97c-6ab912a00f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly pattern detection system and method\n",
      "2\n",
      "Similarity Score: 8.103279113769531, Sentence:  Provided is an anomaly pattern detection system including an anomaly detection device connected to one or more servers.\n",
      "\n",
      "Similarity Score: 8.256790161132812, Sentence: The anomaly detection device may include an anomaly detector configured to model input data by considering all of the input data as normal patterns, and detect an anomaly pattern from the input data based on the modeling result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_model = CrossEncoder(smart_search.cross_encoder_models[0])\n",
    "\n",
    "#scores = [cross_model.predict([query_string,msg]) for msg in topk_df.Abstract]\n",
    "\n",
    "i = 1\n",
    "\n",
    "print(topk_df['Title'][i])\n",
    "\n",
    "sentences = split_paragraph_into_sentences(topk_df['Abstract'][i])\n",
    "\n",
    "print(len(sentences))\n",
    "\n",
    "for sentence in sentences:\n",
    "    score = cross_model.predict([query_string,sentence])\n",
    "    print(f\"Similarity Score: {score}, Sentence: {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2efb87-c928-4cb9-a434-54ed25906d09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common words: {'anomaly', 'detector', 'detection', '.', 'to', 'data', 'an', 'of', ',', 'by', 'device', 'the', 'is', 'one'}\n",
      "Synonym matches: {('detected', 'detect'), ('includes', 'include')}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \"\"\"Returns a set of synonyms for the given word.\"\"\"\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def similar_words_or_synonyms(string1, string2):\n",
    "    \"\"\"Finds and returns similar words or synonyms between two strings.\"\"\"\n",
    "    words1 = nltk.word_tokenize(string1)\n",
    "    words2 = nltk.word_tokenize(string2)\n",
    "\n",
    "    common_words = set(words1) & set(words2)\n",
    "    \n",
    "    synonym_matches = set()\n",
    "\n",
    "    for word1 in words1:\n",
    "        for word2 in words2:\n",
    "            if word1 != word2 and word2 in get_synonyms(word1):\n",
    "                synonym_matches.add((word1, word2))\n",
    "                \n",
    "    return common_words, synonym_matches\n",
    "\n",
    "string1 = \"I am happy and cheerful today.\"\n",
    "string2 = \"It is a joyous and delighted day.\"\n",
    "\n",
    "common, synonyms = similar_words_or_synonyms(query_string, topk_df['Abstract'][i])\n",
    "\n",
    "print(\"Common words:\", common)\n",
    "print(\"Synonym matches:\", synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669df5b5-4dbc-421b-a8c5-9b0a4ad5e1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
